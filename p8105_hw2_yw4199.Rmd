---
title: "p8105_hw2_yw4199"
author: "Yaduo Wang"
date: "2023-10-01"
output: github_document
---
#Problem 2

```{r}
library(tidyverse)
library(readxl)
```

```{r}
trash_data = read_excel("data_files/Trash_Collection_Data.xlsx", sheet = 1, skip = 1)
#specify the sheet in the Excel file and to omit non-data entries
trash_data = janitor::clean_names(trash_data)
#use reasonable variable names

view(trash_data)

Mr.trash = 
  trash_data |> 
  select(dumpster:homes_powered) |> 
  drop_na(dumpster) |> 
#omit rows that do not include dumpster-specific data
  mutate(homes_powered = weight_tons*500/30)
#Update the data to include a new homes_powered variable based on this calculation.

```

```{r}
Pro_data = read_excel("data_files/Trash_Collection_Data.xlsx", sheet = 2, skip = 1)
#specify the sheet in the Excel file and to omit non-data entries
Pro_data = janitor::clean_names(Pro_data)
#use reasonable variable names
view(Pro_data)

Pro.trash = 
  Pro_data |> 
  select(dumpster:homes_powered) |> 
  drop_na(dumpster) |> 
#omit rows that do not include dumpster-specific data
  mutate(homes_powered = weight_tons*500/30)
#Update the data to include a new homes_powered variable based on this calculation.
view(Pro.trash)
```

```{r}
Gwy_data = read_excel("data_files/Trash_Collection_Data.xlsx", sheet = 4, skip = 1)
#specify the sheet in the Excel file and to omit non-data entries
Gwy_data = janitor::clean_names(Gwy_data)
#use reasonable variable names
view(Gwy_data)

Gwy.trash = 
  Gwy_data |> 
  select(dumpster:homes_powered) |> 
  drop_na(dumpster) |> 
#omit rows that do not include dumpster-specific data
  mutate(homes_powered = weight_tons*500/30)
#Update the data to include a new homes_powered variable based on this calculation.
view(Gwy.trash)
```

Combine all three datasets.
```{r}
#add an additional variable to all datasets before combining
Mr.trash = 
  Mr.trash |> 
  mutate(name_sheet = "Mr.Trash Wheel")
Pro.trash = 
  Pro.trash |> 
  mutate(name_sheet = "Professor Trash Wheel") 
Gwy.trash = 
  Gwy.trash |> 
  mutate(name_sheet = "Gwynnda Trash Wheel") 

#keep the variable type constant
Mr.trash$year = as.numeric(Mr.trash$year)
Pro.trash$year = as.numeric(Pro.trash$year)
Gwy.trash$year = as.numeric(Gwy.trash$year)
```

```{r}
#combine all three datasets
three_tidy = 
  bind_rows(Mr.trash, Pro.trash, Gwy.trash) |>
  janitor::clean_names()
view(three_tidy)
```

Description of the data
```{r}
sum_weight_Professor = filter(three_tidy, name_sheet == "Professor Trash Wheel")
sum_gwy = filter(three_tidy, 
                 month == "July", year == 2021, name_sheet =="Gwynnda Trash Wheel")
```


This new dataset contains `r nrow(three_tidy)` rows, and `r ncol(three_tidy)` columns. The columns in this dataset include: `r names(three_tidy)`. The total weight of trash collected by Professor Trash Wheel is `r sum(sum_weight_Professor$weight_tons)`. The total number of cigarette butts collected by Gwynnda in July of 2021 is `r sum(sum_gwy$cigarette_butts)`.


#Problem 3
```{r}
#Import data and clean data
amyloid_data = 
  read_csv(file = "mci_amyloid.csv", skip = 1) |> 
  janitor::clean_names() |> 
  rename("id" = "study_id") |> 
  drop_na(baseline) |> 
  filter(baseline!="Na")
view(amyloid_data)  

baseline_data = 
  read_csv(file = "MCI_baseline.csv", skip = 1)|>
  janitor::clean_names() |> 
  mutate(
    sex = 
      case_match(
        sex, 
        1 ~ "male", 
        0 ~ "female"),
    sex = as.factor(sex)) |> 
   mutate(
    apoe4 = 
      case_match(
        apoe4, 
        1 ~ "APOE4 carrier", 
        0 ~ "APOE4 non-carrier"),
    apoe4 = as.factor(apoe4))
view(baseline_data)
```

First, we import data in the form of csv file, and we dropped the first row. We use reasonable variable names.

How many participants were recruited, and of these how many develop MCI? What is the average baseline age? What proportion of women in the study are APOE4 carriers?
```{r}
#Calculate how many participants recruited.
nrow(baseline_data)
```

```{r}
#develop MCI
develop_MCI = 
  baseline_data |> 
  filter(age_at_onset != "." )
view(age_at_onset)
```

